# ==============================================================================
# MODEL CONFIGURATION
# ==============================================================================

# Student model selection - Uncomment ONE model
# Size estimates are for 4-bit quantization; double for 8-bit
# MODEL="TinyLlama/TinyLlama-1.1B-Chat-v1.0"         # ~1.6 GB
# MODEL="microsoft/Phi-3-mini-4k-instruct"           # ~3.2 GB
MODEL="microsoft/Phi-3-mini-4k-instruct"
#MODEL="meta-llama/Meta-Llama-3-8B-Instruct"          # ~7.5 GB
# MODEL="meta-llama/Llama-2-13b-chat-hf"             # ~11 GB
# MODEL="mistralai/Mixtral-8x7B-Instruct-v0.1"       # ~14 GB

# Quantization settings
# 4 = NF4 QLoRA (default, memory-efficient)
# 8 = Int8 quantization 
# 16 = fp16/bf16 full precision (requires >2x VRAM)
QUANT_BITS=4

# Compute settings
# Use "float16" for NVIDIA 30-series or older without bf16 support
TORCH_COMPUTE_DTYPE="bfloat16"
TORCH_DEVICE="cuda"  # Options: "cuda", "cpu", "mps" (Apple Silicon)
TORCH_COMPILE=false  # Set to true for potential speedup (requires PyTorch 2.0+)

# ==============================================================================
# TEACHER MODELS (via OpenRouter)
# ==============================================================================

# Free tier models at OpenRouter
TEACHER_MODELS="google/gemini-2.5-pro-exp-03-25:free,\
deepseek/deepseek-chat-v3-0324:free,\
deepseek/deepseek-r1:free,\
google/gemini-2.0-flash-exp:free,\
qwen/qwq-32b:free,\
google/gemini-2.0-flash-thinking-exp-1219:free,\
deepseek/deepseek-v3-base:free,\
deepseek/deepseek-r1-zero:free"

# API key for OpenRo"uter
OPENROUTER_API_KEY=""

# ==============================================================================
# LORA CONFIGURATION
# ==============================================================================

# LoRA hyperparameters
LORA_RANK=8                # Rank of the LoRA matrices
LORA_ALPHA=16              # LoRA alpha parameter (scaling)
LORA_DROPOUT=0.05          # LoRA dropout rate
LORA_MODULES="q_proj,v_proj,k_proj,o_proj"  # Target modules

# Continuous learning parameters
THRESHOLD_START=7          # Initial score threshold (0-10)
MIN_THRESHOLD=5            # Minimum threshold to prevent degradation
GOOD_BATCH_SIZE=64         # Number of examples before batch update
FORCE_UPDATE_SECS=100     # Maximum seconds between updates (30 min)
LOW_PASS_WINDOW=200        # Window size for score moving average
LOW_PASS_RATE=0.20         # Acceptance rate threshold for lowering criteria

# Training parameters
TRAIN_BATCH_SIZE=4         # Per-device training batch size
LEARNING_RATE=1e-5         # Learning rate for optimizer
GRAD_ACCUMULATION_STEPS=4  # Gradient accumulation steps
TRAIN_EPOCHS=1             # Epochs per training batch

# ==============================================================================
# SYSTEM CONFIGURATION
# ==============================================================================

# Server settings
API_PORT=8000              # FastAPI server port
REDIS_URL="redis://localhost:6379/0"  # Redis connection URL
WORKERS=1                  # Number of Celery workers

# Logging and storage
LOG_LEVEL="INFO"           # Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_FILE="logs/app.log"    # Main log file
METRICS_FILE="logs/metrics.tsv"  # Performance metrics file
RESPONSES_DIR="responses"  # Directory for teacher responses
ADAPTER_PATH="models/adapter.safetensors"  # LoRA adapter path
